25/12/15 08:41:14 WARN Utils: Your hostname, top-IdeaPad-Gaming-3-15IAH7 resolves to a loopback address: 127.0.1.1; using 192.168.1.164 instead (on interface wlp47s0)
25/12/15 08:41:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2025-12-15 08:41:20,604 - INFO - Initializing Spark session with Hive support...
25/12/15 08:41:20 INFO SparkContext: Running Spark version 3.5.1
25/12/15 08:41:20 INFO SparkContext: OS info Linux, 6.8.0-88-generic, amd64
25/12/15 08:41:20 INFO SparkContext: Java version 21.0.9
25/12/15 08:41:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/12/15 08:41:20 INFO ResourceUtils: ==============================================================
25/12/15 08:41:20 INFO ResourceUtils: No custom resources configured for spark.driver.
25/12/15 08:41:20 INFO ResourceUtils: ==============================================================
25/12/15 08:41:20 INFO SparkContext: Submitted application: BatchProcessingLayer
25/12/15 08:41:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/12/15 08:41:20 INFO ResourceProfile: Limiting resource is cpu
25/12/15 08:41:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/12/15 08:41:20 INFO SecurityManager: Changing view acls to: top
25/12/15 08:41:20 INFO SecurityManager: Changing modify acls to: top
25/12/15 08:41:20 INFO SecurityManager: Changing view acls groups to: 
25/12/15 08:41:20 INFO SecurityManager: Changing modify acls groups to: 
25/12/15 08:41:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: top; groups with view permissions: EMPTY; users with modify permissions: top; groups with modify permissions: EMPTY
25/12/15 08:41:21 INFO Utils: Successfully started service 'sparkDriver' on port 33939.
25/12/15 08:41:21 INFO SparkEnv: Registering MapOutputTracker
25/12/15 08:41:21 INFO SparkEnv: Registering BlockManagerMaster
25/12/15 08:41:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/12/15 08:41:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/12/15 08:41:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/12/15 08:41:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fcdb33c5-04d5-4543-9210-d93ec66dcb96
25/12/15 08:41:21 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB
25/12/15 08:41:21 INFO SparkEnv: Registering OutputCommitCoordinator
25/12/15 08:41:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/12/15 08:41:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/12/15 08:41:21 INFO Executor: Starting executor ID driver on host 192.168.1.164
25/12/15 08:41:21 INFO Executor: OS info Linux, 6.8.0-88-generic, amd64
25/12/15 08:41:21 INFO Executor: Java version 21.0.9
25/12/15 08:41:21 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
25/12/15 08:41:21 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3b816acb for default.
25/12/15 08:41:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45369.
25/12/15 08:41:21 INFO NettyBlockTransferService: Server created on 192.168.1.164:45369
25/12/15 08:41:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/12/15 08:41:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.164, 45369, None)
25/12/15 08:41:21 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.164:45369 with 2.2 GiB RAM, BlockManagerId(driver, 192.168.1.164, 45369, None)
25/12/15 08:41:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.164, 45369, None)
25/12/15 08:41:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.164, 45369, None)
2025-12-15 08:41:22,285 - INFO - ‚úì Spark session with Hive support created
2025-12-15 08:41:22,286 - INFO - Creating Hive database for batch views...
25/12/15 08:41:25 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
25/12/15 08:41:25 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
25/12/15 08:41:29 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
25/12/15 08:41:29 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore top@127.0.1.1
25/12/15 08:41:29 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
25/12/15 08:41:29 WARN ObjectStore: Failed to get database batch_views, returning NoSuchObjectException
25/12/15 08:41:29 WARN ObjectStore: Failed to get database batch_views, returning NoSuchObjectException
25/12/15 08:41:29 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
25/12/15 08:41:29 WARN ObjectStore: Failed to get database batch_views, returning NoSuchObjectException
25/12/15 08:41:30 ERROR log: Got exception: org.apache.hadoop.security.AccessControlException Permission denied: user=top, access=WRITE, inode="/":root:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1855)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1839)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1798)
	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:59)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3175)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:714)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

org.apache.hadoop.security.AccessControlException: Permission denied: user=top, access=WRITE, inode="/":root:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1855)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1839)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1798)
	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:59)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3175)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:714)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2509)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2483)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1485)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1482)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1499)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1474)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2388)
	at org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:538)
	at org.apache.hadoop.hive.metastore.Warehouse.mkdirs(Warehouse.java:194)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database_core(HiveMetaStore.java:880)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:939)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at jdk.proxy2/jdk.proxy2.$Proxy41.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:725)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at jdk.proxy2/jdk.proxy2.$Proxy42.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:434)
	at org.apache.spark.sql.hive.client.Shim_v0_12.createDatabase(HiveShim.scala:574)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createDatabase$1(HiveClientImpl.scala:345)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:342)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createDatabase$1(HiveExternalCatalog.scala:193)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:193)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createDatabase(ExternalCatalogWithListener.scala:47)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:281)
	at org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.createNamespace(V2SessionCatalog.scala:302)
	at org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=top, access=WRITE, inode="/":root:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1855)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1839)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1798)
	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:59)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3175)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:714)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)
	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at jdk.proxy2/jdk.proxy2.$Proxy33.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:674)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at jdk.proxy2/jdk.proxy2.$Proxy34.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2507)
	... 85 more
25/12/15 08:43:45 WARN Utils: Your hostname, top-IdeaPad-Gaming-3-15IAH7 resolves to a loopback address: 127.0.1.1; using 192.168.1.164 instead (on interface wlp47s0)
25/12/15 08:43:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2025-12-15 08:43:46,787 - INFO - Initializing Spark session with Hive support...
25/12/15 08:43:46 INFO SparkContext: Running Spark version 3.5.1
25/12/15 08:43:46 INFO SparkContext: OS info Linux, 6.8.0-88-generic, amd64
25/12/15 08:43:46 INFO SparkContext: Java version 21.0.9
25/12/15 08:43:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/12/15 08:43:47 INFO ResourceUtils: ==============================================================
25/12/15 08:43:47 INFO ResourceUtils: No custom resources configured for spark.driver.
25/12/15 08:43:47 INFO ResourceUtils: ==============================================================
25/12/15 08:43:47 INFO SparkContext: Submitted application: BatchProcessingLayer
25/12/15 08:43:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/12/15 08:43:47 INFO ResourceProfile: Limiting resource is cpu
25/12/15 08:43:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/12/15 08:43:47 INFO SecurityManager: Changing view acls to: top
25/12/15 08:43:47 INFO SecurityManager: Changing modify acls to: top
25/12/15 08:43:47 INFO SecurityManager: Changing view acls groups to: 
25/12/15 08:43:47 INFO SecurityManager: Changing modify acls groups to: 
25/12/15 08:43:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: top; groups with view permissions: EMPTY; users with modify permissions: top; groups with modify permissions: EMPTY
25/12/15 08:43:47 INFO Utils: Successfully started service 'sparkDriver' on port 39801.
25/12/15 08:43:47 INFO SparkEnv: Registering MapOutputTracker
25/12/15 08:43:47 INFO SparkEnv: Registering BlockManagerMaster
25/12/15 08:43:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/12/15 08:43:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/12/15 08:43:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/12/15 08:43:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-859d3939-72f3-41d8-a438-89b4b5d33148
25/12/15 08:43:47 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB
25/12/15 08:43:47 INFO SparkEnv: Registering OutputCommitCoordinator
25/12/15 08:43:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/12/15 08:43:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/12/15 08:43:47 INFO Executor: Starting executor ID driver on host 192.168.1.164
25/12/15 08:43:47 INFO Executor: OS info Linux, 6.8.0-88-generic, amd64
25/12/15 08:43:47 INFO Executor: Java version 21.0.9
25/12/15 08:43:47 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
25/12/15 08:43:47 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@71d1a4d for default.
25/12/15 08:43:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36855.
25/12/15 08:43:47 INFO NettyBlockTransferService: Server created on 192.168.1.164:36855
25/12/15 08:43:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/12/15 08:43:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.164, 36855, None)
25/12/15 08:43:47 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.164:36855 with 2.2 GiB RAM, BlockManagerId(driver, 192.168.1.164, 36855, None)
25/12/15 08:43:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.164, 36855, None)
25/12/15 08:43:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.164, 36855, None)
2025-12-15 08:43:48,316 - INFO - ‚úì Spark session with Hive support created
2025-12-15 08:43:48,317 - INFO - Creating Hive database for batch views...
25/12/15 08:43:51 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
25/12/15 08:43:51 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
25/12/15 08:43:53 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
25/12/15 08:43:53 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore top@127.0.1.1
25/12/15 08:43:53 WARN ObjectStore: Failed to get database batch_views, returning NoSuchObjectException
25/12/15 08:43:53 WARN ObjectStore: Failed to get database batch_views, returning NoSuchObjectException
25/12/15 08:43:53 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
25/12/15 08:43:53 WARN ObjectStore: Failed to get database batch_views, returning NoSuchObjectException
2025-12-15 08:43:53,988 - INFO - ‚úì Hive database 'batch_views' ready
2025-12-15 08:43:53,988 - INFO - ================================================================================
2025-12-15 08:43:53,988 - INFO - READING SOURCE DATA FROM HDFS
2025-12-15 08:43:53,988 - INFO - ================================================================================
2025-12-15 08:43:53,988 - INFO - Reading: hdfs://localhost:8020/data/processed/processed_images.parquet
2025-12-15 08:43:56,324 - INFO - ‚úì Loaded 27000 processed images
2025-12-15 08:43:56,324 - INFO - Reading: hdfs://localhost:8020/data/processed/texture/texture_features.parquet
2025-12-15 08:44:00,626 - INFO - ‚úì Loaded 27000 texture features
2025-12-15 08:44:00,627 - INFO - Joining datasets...
25/12/15 08:44:01 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors
2025-12-15 08:44:06,115 - INFO - ‚úì Enriched dataset: 27000 records
2025-12-15 08:44:06,115 - INFO - ================================================================================
2025-12-15 08:44:06,115 - INFO - COMPUTING CLASS STATISTICS (Batch View)
2025-12-15 08:44:06,115 - INFO - ================================================================================
2025-12-15 08:44:07,957 - INFO - ‚úì Computed statistics for 10 classes
2025-12-15 08:44:07,957 - INFO - Saving to Hive table: batch_views.class_statistics
25/12/15 08:44:08 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
25/12/15 08:44:15 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/12/15 08:44:15 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
25/12/15 08:44:15 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
25/12/15 08:44:15 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
2025-12-15 08:44:15,760 - INFO - ‚úì Class statistics saved to Hive
2025-12-15 08:44:15,761 - INFO - ================================================================================
2025-12-15 08:44:15,761 - INFO - COMPUTING TEMPORAL TRENDS (Batch View)
2025-12-15 08:44:15,761 - INFO - ================================================================================
2025-12-15 08:44:16,260 - INFO - ‚úì Computed trends for 10 periods
2025-12-15 08:44:16,260 - INFO - Saving to Hive table: batch_views.temporal_trends
2025-12-15 08:44:20,486 - INFO - ‚úì Temporal trends saved to Hive
2025-12-15 08:44:20,486 - INFO - ================================================================================
2025-12-15 08:44:20,486 - INFO - COMPUTING CLASS SEPARABILITY (Batch View)
2025-12-15 08:44:20,486 - INFO - ================================================================================
2025-12-15 08:44:24,298 - INFO - ‚úì Computed separability for 45 class pairs
2025-12-15 08:44:24,299 - INFO - Saving to Hive table: batch_views.class_separability
2025-12-15 08:44:27,812 - INFO - ‚úì Class separability saved to Hive
2025-12-15 08:44:27,812 - INFO - ================================================================================
2025-12-15 08:44:27,812 - INFO - COMPUTING FEATURE CORRELATIONS (Batch View)
2025-12-15 08:44:27,812 - INFO - ================================================================================
2025-12-15 08:45:38,714 - INFO - ‚úì Computed 45 feature correlations
2025-12-15 08:45:38,714 - INFO - Saving to Hive table: batch_views.feature_correlations
2025-12-15 08:45:39,442 - INFO - ‚úì Feature correlations saved to Hive
2025-12-15 08:45:39,442 - INFO - ================================================================================
2025-12-15 08:45:39,442 - INFO - BATCH PROCESSING SUMMARY
2025-12-15 08:45:39,442 - INFO - ================================================================================

üìä CLASS STATISTICS (First 5 rows):
+--------------------+-------------+-------------------+------------------+
|          class_name|total_samples|           avg_ndvi|    avg_brightness|
+--------------------+-------------+-------------------+------------------+
|          AnnualCrop|         2500|0.32736023999999986| 894.0287880000019|
|              Forest|         3000|0.37640980000000007|1316.4610466666654|
|HerbaceousVegetation|         2500| 0.4462315599999997|1031.4982240000013|
|             Highway|         2000| 0.6537812999999996| 871.6235749999981|
|          Industrial|         2500|0.37204840000000095|1261.6554720000004|
+--------------------+-------------+-------------------+------------------+
only showing top 5 rows


üìà TEMPORAL TRENDS (First 5 rows):
+-------+--------------------+-------------------+----------------------+
| period|          class_name|           avg_ndvi|avg_texture_complexity|
+-------+--------------------+-------------------+----------------------+
|Month_1|          AnnualCrop|0.32736023999999986|                  NULL|
|Month_2|              Forest|0.37640980000000007|                  NULL|
|Month_3|HerbaceousVegetation| 0.4462315599999997|                  NULL|
|Month_4|             Highway| 0.6537812999999996|                  NULL|
|Month_5|          Industrial|0.37204840000000095|                  NULL|
+-------+--------------------+-------------------+----------------------+
only showing top 5 rows


‚öñÔ∏è CLASS SEPARABILITY (Top 5 most separable pairs):
+------------+-------------+------------------+
|class_name_1| class_name_2|euclidean_distance|
+------------+-------------+------------------+
|  Industrial|      SeaLake|              NULL|
|  Industrial|PermanentCrop|              NULL|
|  Industrial|        River|              NULL|
|  Industrial|  Residential|              NULL|
|  Industrial|      Pasture|              NULL|
+------------+-------------+------------------+
only showing top 5 rows


üîó FEATURE CORRELATIONS (Top 5 strongest):
+----------+----------------+-----------+
| feature_1|       feature_2|correlation|
+----------+----------------+-----------+
| ndvi_mean|   glcm_contrast|        NaN|
|  nir_mean|glcm_homogeneity|        NaN|
|  red_mean|   glcm_contrast|        NaN|
|  nir_mean|     glcm_energy|        NaN|
|green_mean|glcm_homogeneity|        NaN|
+----------+----------------+-----------+
only showing top 5 rows

2025-12-15 08:45:46,183 - INFO - ================================================================================
2025-12-15 08:45:46,183 - INFO - ‚úì BATCH PROCESSING COMPLETE
2025-12-15 08:45:46,183 - INFO - ================================================================================
2025-12-15 08:45:46,183 - INFO - Batch views stored in Hive database: batch_views
2025-12-15 08:45:46,183 - INFO - Tables created:
2025-12-15 08:45:46,183 - INFO -   ‚Ä¢ batch_views.class_statistics
2025-12-15 08:45:46,183 - INFO -   ‚Ä¢ batch_views.temporal_trends
2025-12-15 08:45:46,183 - INFO -   ‚Ä¢ batch_views.class_separability
2025-12-15 08:45:46,183 - INFO -   ‚Ä¢ batch_views.feature_correlations
2025-12-15 08:45:46,183 - INFO - ================================================================================
2025-12-15 08:45:47,172 - INFO - ‚úì Batch processing finished successfully!
2025-12-15 08:45:47,174 - INFO - Closing down clientserver connection
25/12/15 08:53:34 WARN Utils: Your hostname, top-IdeaPad-Gaming-3-15IAH7 resolves to a loopback address: 127.0.1.1; using 192.168.1.164 instead (on interface wlp47s0)
25/12/15 08:53:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2025-12-15 08:53:35,560 - INFO - Initializing Spark session with Hive support...
25/12/15 08:53:35 INFO SparkContext: Running Spark version 3.5.1
25/12/15 08:53:35 INFO SparkContext: OS info Linux, 6.8.0-88-generic, amd64
25/12/15 08:53:35 INFO SparkContext: Java version 21.0.9
25/12/15 08:53:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/12/15 08:53:35 INFO ResourceUtils: ==============================================================
25/12/15 08:53:35 INFO ResourceUtils: No custom resources configured for spark.driver.
25/12/15 08:53:35 INFO ResourceUtils: ==============================================================
25/12/15 08:53:35 INFO SparkContext: Submitted application: BatchProcessingLayer
25/12/15 08:53:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/12/15 08:53:35 INFO ResourceProfile: Limiting resource is cpu
25/12/15 08:53:35 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/12/15 08:53:35 INFO SecurityManager: Changing view acls to: top
25/12/15 08:53:35 INFO SecurityManager: Changing modify acls to: top
25/12/15 08:53:35 INFO SecurityManager: Changing view acls groups to: 
25/12/15 08:53:35 INFO SecurityManager: Changing modify acls groups to: 
25/12/15 08:53:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: top; groups with view permissions: EMPTY; users with modify permissions: top; groups with modify permissions: EMPTY
25/12/15 08:53:35 INFO Utils: Successfully started service 'sparkDriver' on port 40627.
25/12/15 08:53:35 INFO SparkEnv: Registering MapOutputTracker
25/12/15 08:53:35 INFO SparkEnv: Registering BlockManagerMaster
25/12/15 08:53:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/12/15 08:53:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/12/15 08:53:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/12/15 08:53:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-08d17d41-7551-4d80-8d40-3dd3621552cf
25/12/15 08:53:36 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB
25/12/15 08:53:36 INFO SparkEnv: Registering OutputCommitCoordinator
25/12/15 08:53:36 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/12/15 08:53:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/12/15 08:53:36 INFO Executor: Starting executor ID driver on host 192.168.1.164
25/12/15 08:53:36 INFO Executor: OS info Linux, 6.8.0-88-generic, amd64
25/12/15 08:53:36 INFO Executor: Java version 21.0.9
25/12/15 08:53:36 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
25/12/15 08:53:36 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3b816acb for default.
25/12/15 08:53:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38381.
25/12/15 08:53:36 INFO NettyBlockTransferService: Server created on 192.168.1.164:38381
25/12/15 08:53:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/12/15 08:53:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.164, 38381, None)
25/12/15 08:53:36 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.164:38381 with 2.2 GiB RAM, BlockManagerId(driver, 192.168.1.164, 38381, None)
25/12/15 08:53:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.164, 38381, None)
25/12/15 08:53:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.164, 38381, None)
2025-12-15 08:53:36,461 - INFO - ‚úì Spark session with Hive support created
2025-12-15 08:53:36,461 - INFO - Creating Hive database for batch views...
25/12/15 08:53:37 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
25/12/15 08:53:37 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
25/12/15 08:53:38 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
25/12/15 08:53:38 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore top@127.0.1.1
25/12/15 08:53:38 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
2025-12-15 08:53:38,977 - INFO - ‚úì Hive database 'batch_views' ready
2025-12-15 08:53:38,977 - INFO - ================================================================================
2025-12-15 08:53:38,977 - INFO - READING SOURCE DATA FROM HDFS
2025-12-15 08:53:38,977 - INFO - ================================================================================
2025-12-15 08:53:38,977 - INFO - Reading: hdfs://localhost:8020/data/processed/processed_images.parquet
2025-12-15 08:53:40,225 - INFO - ‚úì Loaded 27000 processed images
2025-12-15 08:53:40,226 - INFO - Reading: hdfs://localhost:8020/data/processed/texture/texture_features.parquet
2025-12-15 08:53:41,995 - INFO - ‚úì Loaded 27000 texture features
2025-12-15 08:53:41,996 - INFO - Joining datasets...
2025-12-15 08:53:44,925 - INFO - ‚úì Enriched dataset: 27000 records
2025-12-15 08:53:44,925 - INFO - ================================================================================
2025-12-15 08:53:44,925 - INFO - COMPUTING CLASS STATISTICS (Batch View)
2025-12-15 08:53:44,925 - INFO - ================================================================================
2025-12-15 08:53:46,151 - INFO - ‚úì Computed statistics for 10 classes
2025-12-15 08:53:46,151 - INFO - Saving to Hive table: batch_views.class_statistics
25/12/15 08:53:47 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
25/12/15 08:53:47 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors
25/12/15 08:53:50 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/12/15 08:53:50 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
25/12/15 08:53:50 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
25/12/15 08:53:50 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
2025-12-15 08:53:50,893 - INFO - ‚úì Class statistics saved to Hive
2025-12-15 08:53:50,893 - INFO - ================================================================================
2025-12-15 08:53:50,893 - INFO - COMPUTING TEMPORAL TRENDS (Batch View)
2025-12-15 08:53:50,893 - INFO - ================================================================================
2025-12-15 08:53:51,154 - INFO - ‚úì Computed trends for 10 periods
2025-12-15 08:53:51,154 - INFO - Saving to Hive table: batch_views.temporal_trends
2025-12-15 08:53:54,121 - INFO - ‚úì Temporal trends saved to Hive
2025-12-15 08:53:54,121 - INFO - ================================================================================
2025-12-15 08:53:54,121 - INFO - COMPUTING CLASS SEPARABILITY (Batch View)
2025-12-15 08:53:54,121 - INFO - ================================================================================
2025-12-15 08:53:56,165 - INFO - ‚úì Computed separability for 45 class pairs
2025-12-15 08:53:56,165 - INFO - Saving to Hive table: batch_views.class_separability
2025-12-15 08:53:58,224 - INFO - ‚úì Class separability saved to Hive
2025-12-15 08:53:58,224 - INFO - ================================================================================
2025-12-15 08:53:58,224 - INFO - COMPUTING FEATURE CORRELATIONS (Batch View)
2025-12-15 08:53:58,224 - INFO - ================================================================================
2025-12-15 08:54:32,807 - INFO - ‚úì Computed 45 feature correlations
2025-12-15 08:54:32,807 - INFO - Saving to Hive table: batch_views.feature_correlations
2025-12-15 08:54:33,526 - INFO - ‚úì Feature correlations saved to Hive
2025-12-15 08:54:33,526 - INFO - ================================================================================
2025-12-15 08:54:33,526 - INFO - BATCH PROCESSING SUMMARY
2025-12-15 08:54:33,526 - INFO - ================================================================================

üìä CLASS STATISTICS (First 5 rows):
+--------------------+-------------+-------------------+------------------+
|          class_name|total_samples|           avg_ndvi|    avg_brightness|
+--------------------+-------------+-------------------+------------------+
|          AnnualCrop|         2500|0.32736023999999986| 894.0287880000019|
|              Forest|         3000|0.37640980000000007|1316.4610466666654|
|HerbaceousVegetation|         2500| 0.4462315599999997|1031.4982240000013|
|             Highway|         2000| 0.6537812999999996| 871.6235749999981|
|          Industrial|         2500|0.37204840000000095|1261.6554720000004|
+--------------------+-------------+-------------------+------------------+
only showing top 5 rows


üìà TEMPORAL TRENDS (First 5 rows):
+-------+--------------------+-------------------+----------------------+
| period|          class_name|           avg_ndvi|avg_texture_complexity|
+-------+--------------------+-------------------+----------------------+
|Month_1|          AnnualCrop|0.32736023999999986|                  NULL|
|Month_2|              Forest|0.37640980000000007|                  NULL|
|Month_3|HerbaceousVegetation| 0.4462315599999997|                  NULL|
|Month_4|             Highway| 0.6537812999999996|                  NULL|
|Month_5|          Industrial|0.37204840000000095|                  NULL|
+-------+--------------------+-------------------+----------------------+
only showing top 5 rows


‚öñÔ∏è CLASS SEPARABILITY (Top 5 most separable pairs):
+------------+-------------+------------------+
|class_name_1| class_name_2|euclidean_distance|
+------------+-------------+------------------+
|  Industrial|      SeaLake|              NULL|
|  Industrial|PermanentCrop|              NULL|
|  Industrial|        River|              NULL|
|  Industrial|  Residential|              NULL|
|  Industrial|      Pasture|              NULL|
+------------+-------------+------------------+
only showing top 5 rows


üîó FEATURE CORRELATIONS (Top 5 strongest):
+----------+----------------+-----------+
| feature_1|       feature_2|correlation|
+----------+----------------+-----------+
|  red_mean|   glcm_contrast|        NaN|
| ndvi_mean|   glcm_contrast|        NaN|
|green_mean|glcm_homogeneity|        NaN|
| ndvi_mean|glcm_homogeneity|        NaN|
|  nir_mean|glcm_homogeneity|        NaN|
+----------+----------------+-----------+
only showing top 5 rows

2025-12-15 08:54:36,859 - INFO - ================================================================================
2025-12-15 08:54:36,859 - INFO - ‚úì BATCH PROCESSING COMPLETE
2025-12-15 08:54:36,859 - INFO - ================================================================================
2025-12-15 08:54:36,859 - INFO - Batch views stored in Hive database: batch_views
2025-12-15 08:54:36,859 - INFO - Tables created:
2025-12-15 08:54:36,859 - INFO -   ‚Ä¢ batch_views.class_statistics
2025-12-15 08:54:36,859 - INFO -   ‚Ä¢ batch_views.temporal_trends
2025-12-15 08:54:36,859 - INFO -   ‚Ä¢ batch_views.class_separability
2025-12-15 08:54:36,859 - INFO -   ‚Ä¢ batch_views.feature_correlations
2025-12-15 08:54:36,859 - INFO - ================================================================================
2025-12-15 08:54:37,854 - INFO - ‚úì Batch processing finished successfully!
2025-12-15 08:54:37,854 - INFO - Closing down clientserver connection
